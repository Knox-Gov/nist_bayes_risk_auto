import argparse
import csv
import json
import logging
import sys
import os
import time
import random
from typing import Dict, Any, Optional

try:
    import boto3
    from botocore.exceptions import ClientError
except ImportError:
    print("Error: 'boto3' library not found. Please install it using:")
    print("pip install -r requirements.txt")
    sys.exit(1)

# --- Constants ---
PRIMARY_PROMPT_TEMPLATE = """
You are an expert cybersecurity risk analyst specializing in NIST 800-53 R5 and quantitative risk assessment using Bayesian principles.

For the following NIST 800-53 R5 control:
Control Family: {control_family}
Control ID: {control_id}
Control Name: {control_name}
Control Description: {control_description}

{revision_request}

Perform a Bayesian risk quantification analysis. Define a plausible primary risk scenario this control mitigates in a typical, moderately secure enterprise environment. Then, provide the following components:
1.  Risk Scenario Description: Concise description.
2.  Event (A) Description: Specific negative event.
3.  Prior Probability P(A): Annual probability (0.0-1.0) of Event (A) before considering this control.
4.  Rationale for P(A): Reasoning for P(A).
5.  Evidence (B) Description: State where this control is effectively implemented.
6.  Probability of Evidence P(B): Probability (0.0-1.0) control is effectively implemented (Evidence B exists).
7.  Rationale for P(B): Reasoning for P(B).
8.  Likelihood P(B|A): Probability (0.0-1.0) control *was* effectively implemented *given* Event (A) occurred. (Often low).
9.  Rationale for P(B|A): Reasoning for P(B|A).

Output the response ONLY as a JSON object containing keys: "scenario", "event_A", "P_A", "rationale_PA", "evidence_B", "P_B", "rationale_PB", "P_B_given_A", "rationale_PB_given_A". Probabilities must be numerical values between 0.0 and 1.0.
"""

QA_PROMPT_TEMPLATE = """
You are a meticulous cybersecurity risk management expert reviewing a Bayesian risk analysis generated by another analyst for a NIST 800-53 R5 control.

Control ID: {control_id}
Analysis Attempt: {attempt_count}

Analysis Provided by Primary Analyst:
{primary_analysis_json_pretty_print}

Review the provided analysis for plausibility, internal consistency, and relevance to the control within a typical enterprise context. Check if the probabilities are reasonable estimations (between 0.0 and 1.0) and if the rationales support the values. Ensure the output is valid JSON matching the requested structure.

Respond with ONLY "OK" if the analysis is acceptable and plausible without major flaws.
If there are significant issues (e.g., illogical scenario, wildly implausible probabilities, contradictory rationale, irrelevant analysis, invalid JSON, probabilities outside 0.0-1.0), respond with a brief, actionable description of the main issue(s) to help the analyst revise. Do NOT respond "OK" if you find major issues.
"""

OUTPUT_HEADERS = [
    "Family", "ID", "Control Name", "Risk Scenario Description",
    "Event (A) Description", "Prior Probability P(A)", "Rationale for P(A)",
    "Evidence (B) Description", "Probability of Evidence P(B)",
    "Rationale for P(B)", "Likelihood P(B|A)", "Rationale for P(B|A)",
    "Posterior Probability P(A|B)", "QA Status", "QA Confidence/Critique",
    "Attempts", "Notes/Assumptions"
]

# --- Bedrock Interaction ---


def invoke_bedrock_model(
    client: Any,
    model_id: str,
    prompt: str,
    system_prompt: Optional[str] = None,
    max_retries: int = 5,
    initial_backoff: float = 1.0,
    max_backoff: float = 30.0,
    jitter: float = 0.1
) -> Optional[str]:
    """
    Invokes an AWS Bedrock model (specifically handling Anthropic Claude format).

    Args:
        client: Initialized boto3 Bedrock Runtime client.
        model_id: The ID of the Bedrock model to invoke.
        prompt: The main user prompt for the model.
        system_prompt: An optional system prompt (used by some models like Claude 3).
        max_retries: Maximum number of retry attempts for throttling errors.
        initial_backoff: Initial backoff time in seconds.
        max_backoff: Maximum backoff time in seconds.
        jitter: Random jitter factor to add to backoff (0.0-1.0).

    Returns:
        The text response from the model, or None if an error occurred.
    """
    retry_count = 0
    backoff = initial_backoff
    
    while retry_count <= max_retries:
        try:
            messages = [{"role": "user", "content": prompt}]
            body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2048,  # Adjust as needed
                "messages": messages,
                "temperature": 0.5,  # Adjust for desired creativity/determinism
                "top_p": 0.9,
            }
            if system_prompt:
                body["system"] = system_prompt

            logging.debug(f"Invoking model {model_id} with body: {json.dumps(body, indent=2)}")

            response = client.invoke_model(
                body=json.dumps(body),
                modelId=model_id,
                contentType='application/json',
                accept='application/json'
            )
            response_body = json.loads(response['body'].read())

            logging.debug(f"Raw response from {model_id}: {json.dumps(response_body, indent=2)}")

            # Check for potential errors or unexpected structures
            if 'content' not in response_body or not isinstance(response_body['content'], list) or not response_body['content']:
                logging.error(f"Unexpected response structure from {model_id}: {response_body}")
                return None

            # Extract text content - assuming the primary text is in the first content block
            if response_body['content'][0].get('type') == 'text':
                model_response = response_body['content'][0]['text']
                logging.debug(f"Extracted text response from {model_id}: {model_response}")
                return model_response.strip()
            else:
                logging.error(f"First content block is not text in response from {model_id}: {response_body['content'][0]}")
                return None

        except ClientError as e:
            error_code = getattr(e, 'response', {}).get('Error', {}).get('Code', '')
            if error_code == 'ThrottlingException' and retry_count < max_retries:
                retry_count += 1
                # Calculate backoff with jitter
                jitter_value = backoff * jitter * random.uniform(-1, 1)
                sleep_time = min(max_backoff, backoff + jitter_value)
                if sleep_time < 0:
                    sleep_time = 0.1  # Ensure we don't get negative sleep time
                
                logging.warning(f"Throttling error for {model_id}, attempt {retry_count}/{max_retries}. Retrying in {sleep_time:.2f} seconds...")
                print(f"Rate limit hit, retrying in {sleep_time:.2f} seconds... (attempt {retry_count}/{max_retries})")
                time.sleep(sleep_time)
                
                # Exponential backoff for next attempt
                backoff = min(max_backoff, backoff * 2)
            else:
                # Non-throttling error or max retries exceeded
                logging.error(f"AWS Bedrock ClientError for model {model_id}: {e}")
                print(f"Error invoking {model_id}: {e}", file=sys.stderr)
                return None
                
        except json.JSONDecodeError as e:
            logging.error(f"Failed to decode JSON response from Bedrock for model {model_id}: {e}")
            print(f"Error parsing Bedrock response for {model_id}.", file=sys.stderr)
            return None
            
        except Exception as e:
            logging.error(f"Unexpected error invoking Bedrock model {model_id}: {e}", exc_info=True)
            print(f"Unexpected error invoking {model_id}: {e}", file=sys.stderr)
            return None
    
    # If we've exhausted all retries
    logging.error(f"Failed to invoke {model_id} after {max_retries} retry attempts due to throttling")
    print(f"Error: Failed to invoke {model_id} after {max_retries} retry attempts due to throttling", file=sys.stderr)
    return None


def parse_primary_analysis(response_text: str) -> Optional[Dict[str, Any]]:
    """Parses the JSON response from the primary analysis LLM."""
    try:
        # Sometimes the LLM might wrap the JSON in backticks or comments
        json_start = response_text.find('{')
        json_end = response_text.rfind('}')
        if json_start != -1 and json_end != -1:
            potential_json = response_text[json_start:json_end+1]
            analysis = json.loads(potential_json)

            # Basic validation
            required_keys = {"scenario", "event_A", "P_A", "rationale_PA", "evidence_B",
                             "P_B", "rationale_PB", "P_B_given_A", "rationale_PB_given_A"}
            if not required_keys.issubset(analysis.keys()):
                logging.warning(f"Primary analysis JSON missing required keys. Found: {analysis.keys()}")
                return None  # Let QA catch this

            # Type check probabilities - convert if possible, error if not
            for key in ["P_A", "P_B", "P_B_given_A"]:
                val = analysis.get(key)
                if isinstance(val, (int, float)):
                    if 0.0 <= val <= 1.0:
                        analysis[key] = float(val)
                    else:
                        logging.warning(f"Probability {key}={val} out of range [0.0, 1.0].")
                        # Allow QA to flag this, don't fail parsing here yet
                        # analysis[key] = None # Or set to None if strict parsing needed
                elif isinstance(val, str):
                    try:
                        float_val = float(val)
                        if 0.0 <= float_val <= 1.0:
                            analysis[key] = float_val
                        else:
                            logging.warning(f"Probability {key}='{val}' out of range [0.0, 1.0].")
                            # Allow QA to flag
                            # analysis[key] = None
                    except ValueError:
                        logging.error(f"Could not convert probability {key}='{val}' to float.")
                        return None  # Hard parsing failure
                else:
                    logging.error(f"Invalid type for probability {key}: {type(val)}. Value: {val}")
                    return None  # Hard parsing failure

            return analysis
        else:
            logging.error(f"Could not find valid JSON object in primary response: {response_text}")
            return None
    except json.JSONDecodeError as e:
        logging.error(f"Failed to parse primary analysis JSON: {e}. Response text: {response_text}")
        return None
    except Exception as e:
        logging.error(f"Unexpected error parsing primary analysis: {e}", exc_info=True)
        return None


def calculate_posterior(p_a: Optional[float], p_b: Optional[float], p_b_given_a: Optional[float]) -> Optional[float | str]:
    """Calculates the posterior probability P(A|B)."""
    if p_a is None or p_b is None or p_b_given_a is None:
        logging.warning("Cannot calculate posterior: One or more input probabilities are missing.")
        return "N/A (Missing Input)"

    if not all(isinstance(p, (int, float)) and 0.0 <= p <= 1.0 for p in [p_a, p_b, p_b_given_a]):
         logging.warning(f"Invalid probability types or values for posterior calc: P(A)={p_a}, P(B)={p_b}, P(B|A)={p_b_given_a}")
         return "N/A (Invalid Input)"

    if p_b == 0.0:
        logging.warning("Cannot calculate posterior: P(B) is zero.")
        return "N/A (P(B)=0)"
    try:
        posterior = (p_b_given_a * p_a) / p_b
        # Clamp posterior between 0 and 1, as minor LLM inaccuracies could push it slightly outside
        return max(0.0, min(1.0, posterior))
    except ZeroDivisionError:
        # This case should be caught by the p_b == 0 check, but just in case
        logging.error("ZeroDivisionError during posterior calculation, despite P(B) check.")
        return "N/A (Division Error)"
    except Exception as e:
        logging.error(f"Unexpected error during posterior calculation: {e}", exc_info=True)
        return "N/A (Calculation Error)"


# --- Main Processing Logic ---

def process_controls(args: argparse.Namespace):
    """
    Main function to process controls, invoke LLMs, perform QA, and write results.
    """
    logging.info("Starting NIST Bayesian Risk Analysis.")
    logging.info(f"Input file: {args.input_file}")
    logging.info(f"Output file: {args.output_file}")
    logging.info(f"Log file: {args.log_file}")
    logging.info(f"Primary Model ID: {args.primary_model_id}")
    logging.info(f"QA Model ID: {args.qa_model_id}")
    logging.info(f"Max Attempts: {args.max_attempts}")
    
    # Add delay between controls to avoid rate limiting
    control_delay = args.control_delay
    logging.info(f"Delay between controls: {control_delay} seconds")

    # Initialize AWS Bedrock client
    try:
        # Explicitly set region if needed, otherwise relies on default config
        # session = boto3.Session(region_name='us-east-1') # Example region
        # bedrock_client = session.client('bedrock-runtime')
        bedrock_client = boto3.client('bedrock-runtime')
        # Test connection (optional, but good practice)
        # bedrock_client.list_foundation_models()
        logging.info("Successfully initialized Boto3 Bedrock Runtime client.")
    except ClientError as e:
        logging.error(f"Failed to initialize Boto3 Bedrock client: {e}. Ensure AWS credentials and permissions are configured correctly.", exc_info=True)
        print(f"Error: Failed to connect to AWS Bedrock. Check credentials and permissions. Details in '{args.log_file}'.", file=sys.stderr)
        sys.exit(1)
    except Exception as e: # Catch potential issues beyond ClientError during init
        logging.error(f"Unexpected error initializing Boto3 client: {e}", exc_info=True)
        print(f"Error: Unexpected error during AWS initialization. Details in '{args.log_file}'.", file=sys.stderr)
        sys.exit(1)


    # Prepare output file
    try:
        output_dir = os.path.dirname(args.output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
            logging.info(f"Created output directory: {output_dir}")

        with open(args.output_file, 'w', newline='', encoding='utf-8') as outfile:
            writer = csv.writer(outfile)
            writer.writerow(OUTPUT_HEADERS)
            logging.info(f"Output CSV file '{args.output_file}' initialized with headers.")
    except IOError as e:
        logging.error(f"Failed to open or write headers to output file {args.output_file}: {e}", exc_info=True)
        print(f"Error: Could not write to output file '{args.output_file}'. Check permissions and path.", file=sys.stderr)
        sys.exit(1)

    # Read and process input controls
    processed_count = 0
    error_count = 0
    try:
        with open(args.input_file, 'r', newline='', encoding='utf-8') as infile:
            reader = csv.DictReader(infile)
            if not all(h in reader.fieldnames for h in ["Family", "ID", "Control Name", "NIST Control Description"]):
                 missing = set(["Family", "ID", "Control Name", "NIST Control Description"]) - set(reader.fieldnames or [])
                 logging.error(f"Input CSV missing required headers: {missing}")
                 print(f"Error: Input file '{args.input_file}' is missing required headers: {missing}", file=sys.stderr)
                 sys.exit(1)

            for row_num, row in enumerate(reader, start=2): # start=2 for header row + 1-based data rows
                control_family = row.get("Family", "").strip()
                control_id = row.get("ID", "").strip()
                control_name = row.get("Control Name", "").strip()
                control_description = row.get("NIST Control Description", "").strip()

                if not all([control_family, control_id, control_name, control_description]):
                    logging.warning(f"Skipping row {row_num} due to missing required data: {row}")
                    print(f"Warning: Skipping row {row_num} in '{args.input_file}' due to missing data.", file=sys.stderr)
                    continue

                logging.info(f"--- Processing Control: {control_id} ({control_name}) ---")
                print(f"Processing {control_id}...")

                attempt_count = 0
                qa_status = "Pending"
                last_qa_critique = None
                final_primary_analysis: Optional[Dict[str, Any]] = None
                current_primary_analysis: Optional[Dict[str, Any]] = None # Store analysis from current attempt

                while attempt_count < args.max_attempts and qa_status != "Approved":
                    attempt_count += 1
                    print(f"  Attempt {attempt_count}/{args.max_attempts} for {control_id}...")
                    logging.info(f"Control {control_id}: Attempt {attempt_count}")

                    # 1. Construct Primary Prompt
                    revision_request = ""
                    if attempt_count > 1 and last_qa_critique:
                        revision_request = f'PREVIOUS ATTEMPT FEEDBACK: The previous analysis received the following critique from a QA reviewer: "{last_qa_critique}". Please generate a revised analysis addressing these points, ensuring output is valid JSON with probabilities between 0.0 and 1.0.'

                    primary_prompt = PRIMARY_PROMPT_TEMPLATE.format(
                        control_family=control_family,
                        control_id=control_id,
                        control_name=control_name,
                        control_description=control_description,
                        revision_request=revision_request
                    )

                    # 2. Invoke Primary LLM
                    primary_response_text = invoke_bedrock_model(
                        bedrock_client, args.primary_model_id, primary_prompt
                    )

                    if not primary_response_text:
                        logging.error(f"Control {control_id}, Attempt {attempt_count}: Failed to get response from primary model {args.primary_model_id}.")
                        qa_status = "Error - Primary LLM Failed"
                        last_qa_critique = "Primary LLM API call failed or returned empty response."
                        break # Break inner loop for this control

                    # 3. Parse Primary Response
                    current_primary_analysis = parse_primary_analysis(primary_response_text)
                    if not current_primary_analysis:
                        logging.error(f"Control {control_id}, Attempt {attempt_count}: Failed to parse valid JSON from primary model response.")
                        qa_status = "Error - Primary LLM Invalid Response"
                        last_qa_critique = "Primary LLM response was not valid JSON or missed required fields/structure. Raw response logged."
                        # Log the raw response for debugging if parsing fails
                        logging.debug(f"Control {control_id}, Attempt {attempt_count}: Raw Primary Response causing parse failure:\n{primary_response_text}")
                        # We don't break here, let QA try to flag it if possible, or max attempts will hit.
                        # However, we need *something* to potentially write later, so keep the last good one if any.
                        # If this is the first attempt, final_primary_analysis remains None.
                    else:
                        # Successfully parsed, store this as the latest potentially good analysis
                        final_primary_analysis = current_primary_analysis
                        logging.info(f"Control {control_id}, Attempt {attempt_count}: Successfully parsed primary analysis.")


                    # 4. Construct QA Prompt (only if primary parsing worked)
                    if not current_primary_analysis:
                         # If primary parsing failed, we can't QA it.
                         # The error status is already set. We'll let the loop continue
                         # to potentially hit max attempts or if a *previous* attempt
                         # was OK'd (unlikely scenario, but handles edge cases).
                         # Set critique to reflect the parsing failure.
                         last_qa_critique = "Primary LLM response parsing failed. Cannot perform QA."
                         logging.warning(f"Control {control_id}, Attempt {attempt_count}: Skipping QA due to primary analysis parsing failure.")
                         continue # Skip QA for this attempt, proceed to next attempt or loop end


                    try:
                        primary_analysis_pretty = json.dumps(current_primary_analysis, indent=2)
                    except Exception as json_err:
                         logging.error(f"Control {control_id}, Attempt {attempt_count}: Error pretty-printing primary JSON for QA prompt: {json_err}")
                         primary_analysis_pretty = str(current_primary_analysis) # Fallback

                    qa_prompt = QA_PROMPT_TEMPLATE.format(
                        control_id=control_id,
                        attempt_count=attempt_count,
                        primary_analysis_json_pretty_print=primary_analysis_pretty
                    )

                    # 5. Invoke QA LLM (with a small delay to avoid rate limiting)
                    if args.api_call_delay > 0:
                        logging.debug(f"Waiting {args.api_call_delay} seconds before QA API call to avoid rate limiting...")
                        time.sleep(args.api_call_delay)
                        
                    qa_response_text = invoke_bedrock_model(
                        bedrock_client, args.qa_model_id, qa_prompt
                    )

                    if not qa_response_text:
                        logging.error(f"Control {control_id}, Attempt {attempt_count}: Failed to get response from QA model {args.qa_model_id}.")
                        # Don't change overall status drastically, but log issue.
                        # Keep last known critique if any, otherwise add this failure note.
                        last_qa_critique = (last_qa_critique or "") + " | QA LLM API call failed."
                        # Continue loop, maybe primary analysis was actually ok and QA fails unrelatedly.
                        continue

                    # 6. Process QA Response
                    current_qa_response = qa_response_text.strip()
                    logging.info(f"Control {control_id}, Attempt {attempt_count}: QA Response: '{current_qa_response}'")

                    if current_qa_response.upper() == "OK":
                        qa_status = "Approved"
                        last_qa_critique = "OK"
                        logging.info(f"Control {control_id}: QA Approved on attempt {attempt_count}.")
                        print(f"  Control {control_id}: QA Approved on attempt {attempt_count}.")
                        # Keep the current_primary_analysis as the final one since it passed QA
                        final_primary_analysis = current_primary_analysis
                        break # Exit the while loop successfully
                    else:
                        qa_status = "Failed - QA Critique" # Tentative status
                        last_qa_critique = current_qa_response
                        logging.warning(f"Control {control_id}, Attempt {attempt_count}: QA Failed. Critique: {last_qa_critique}")
                        # Continue loop for refinement if attempts remain

                # --- End of QA Loop for one control ---

                # 7. Final Status Check after loop
                if qa_status == "Pending" or qa_status == "Failed - QA Critique": # Loop finished due to max attempts
                     if attempt_count >= args.max_attempts:
                         qa_status = "Failed - Max Attempts Reached"
                         logging.warning(f"Control {control_id}: Failed QA after {args.max_attempts} attempts. Last critique: {last_qa_critique}")
                         print(f"  Control {control_id}: Failed QA after {args.max_attempts} attempts.")


                # 8. Calculate Posterior Probability (using the *final* analysis)
                posterior_p = "N/A (No Analysis)"
                if final_primary_analysis: # Use the last *successfully parsed* or *QA approved* analysis
                     p_a = final_primary_analysis.get("P_A")
                     p_b = final_primary_analysis.get("P_B")
                     p_b_given_a = final_primary_analysis.get("P_B_given_A")
                     posterior_p = calculate_posterior(p_a, p_b, p_b_given_a)
                     logging.info(f"Control {control_id}: Calculated Posterior P(A|B) = {posterior_p}")
                else:
                     # This happens if primary LLM failed on all attempts or never produced parsable JSON
                     logging.error(f"Control {control_id}: No valid primary analysis available to calculate posterior probability.")
                     if qa_status not in ["Error - Primary LLM Failed", "Error - Primary LLM Invalid Response"]:
                         # If status wasn't already an error, mark it as such
                          qa_status = "Error - No Valid Analysis"
                          last_qa_critique = (last_qa_critique or "") + " | No valid primary analysis was generated."


                # 9. Prepare Output Row
                notes = f"Analysis based on Bedrock models (Primary: {args.primary_model_id}, QA: {args.qa_model_id}). Assumes typical enterprise context. Max Attempts: {args.max_attempts}."
                output_row = {
                    "Family": control_family,
                    "ID": control_id,
                    "Control Name": control_name,
                    "Risk Scenario Description": final_primary_analysis.get("scenario", "") if final_primary_analysis else "N/A",
                    "Event (A) Description": final_primary_analysis.get("event_A", "") if final_primary_analysis else "N/A",
                    "Prior Probability P(A)": final_primary_analysis.get("P_A", "") if final_primary_analysis else "N/A",
                    "Rationale for P(A)": final_primary_analysis.get("rationale_PA", "") if final_primary_analysis else "N/A",
                    "Evidence (B) Description": final_primary_analysis.get("evidence_B", "") if final_primary_analysis else "N/A",
                    "Probability of Evidence P(B)": final_primary_analysis.get("P_B", "") if final_primary_analysis else "N/A",
                    "Rationale for P(B)": final_primary_analysis.get("rationale_PB", "") if final_primary_analysis else "N/A",
                    "Likelihood P(B|A)": final_primary_analysis.get("P_B_given_A", "") if final_primary_analysis else "N/A",
                    "Rationale for P(B|A)": final_primary_analysis.get("rationale_PB_given_A", "") if final_primary_analysis else "N/A",
                    "Posterior Probability P(A|B)": f"{posterior_p:.4f}" if isinstance(posterior_p, float) else posterior_p,
                    "QA Status": qa_status,
                    "QA Confidence/Critique": last_qa_critique or "",
                    "Attempts": attempt_count,
                    "Notes/Assumptions": notes
                }

                # 10. Write Row to Output CSV
                try:
                    with open(args.output_file, 'a', newline='', encoding='utf-8') as outfile:
                        writer = csv.DictWriter(outfile, fieldnames=OUTPUT_HEADERS)
                        writer.writerow(output_row)
                    processed_count += 1
                    if qa_status.startswith("Error"):
                        error_count += 1

                except IOError as e:
                    logging.error(f"Error writing row for control {control_id} to {args.output_file}: {e}", exc_info=True)
                    print(f"Error: Failed to write data for control {control_id} to output file. See log.", file=sys.stderr)
                    error_count += 1 # Count as error if writing fails
                except Exception as e:
                    logging.error(f"Unexpected error preparing or writing row for control {control_id}: {e}", exc_info=True)
                    print(f"Error: Unexpected error processing control {control_id}. See log.", file=sys.stderr)
                    error_count += 1

                time.sleep(control_delay)

    except FileNotFoundError:
        logging.error(f"Input file not found: {args.input_file}")
        print(f"Error: Input file '{args.input_file}' not found.", file=sys.stderr)
        sys.exit(1)
    except IOError as e:
        logging.error(f"Error reading input file {args.input_file}: {e}", exc_info=True)
        print(f"Error: Could not read input file '{args.input_file}'. Check permissions and format.", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        logging.error(f"An unexpected error occurred during processing: {e}", exc_info=True)
        print(f"An unexpected error occurred. Check '{args.log_file}' for details.", file=sys.stderr)

    finally:
        final_message = f"Analysis complete. Processed {processed_count} controls."
        if error_count > 0:
            final_message += f" Encountered errors on {error_count} controls."
        logging.info(final_message)
        print(final_message)
        print(f"Results saved to: {args.output_file}")
        print(f"Detailed logs saved to: {args.log_file}")


# --- Argument Parsing and Execution ---

def main():
    parser = argparse.ArgumentParser(
        description="Automate Bayesian risk quantification for NIST 800-53 R5 controls using AWS Bedrock LLMs.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        "--input-file",
        type=str,
        default="controls.csv",
        help="Path to the input CSV file containing NIST controls (Columns: Family, ID, Control Name, NIST Control Description)."
    )
    parser.add_argument(
        "--output-file",
        type=str,
        default="nist_bayes_analysis_results.csv",
        help="Path to the output CSV file for all analyses."
    )
    parser.add_argument(
        "--log-file",
        type=str,
        default="analysis.log",
        help="Path to the log file."
    )
    parser.add_argument(
        "--primary-model-id",
        type=str,
        required=True,
        help="AWS Bedrock model ID for the primary analysis LLM (e.g., anthropic.claude-3-sonnet-20240229-v1:0)."
    )
    parser.add_argument(
        "--qa-model-id",
        type=str,
        required=True,
        help="AWS Bedrock model ID for the QA LLM (e.g., anthropic.claude-3-haiku-20240307-v1:0)."
    )
    parser.add_argument(
        "--max-attempts",
        type=int,
        default=5,
        help="Maximum number of refinement attempts per control if QA fails (must be >= 1)."
    )
    parser.add_argument(
        "--control-delay",
        type=float,
        default=0.0,
        help="Delay in seconds between processing controls (to avoid rate limiting)."
    )
    parser.add_argument(
        "--api-call-delay",
        type=float,
        default=0.0,
        help="Delay in seconds between API calls (to avoid rate limiting)."
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Set the logging level."
    )

    args = parser.parse_args()

    # Validate max_attempts
    if args.max_attempts < 1:
        print("Error: --max-attempts must be 1 or greater.", file=sys.stderr)
        sys.exit(1)

    # --- Logging Setup ---
    log_level = getattr(logging, args.log_level.upper(), logging.INFO)
    log_format = '%(asctime)s - %(levelname)s - %(message)s'

    # Ensure log directory exists
    log_dir = os.path.dirname(args.log_file)
    if log_dir and not os.path.exists(log_dir):
        try:
            os.makedirs(log_dir)
        except OSError as e:
            print(f"Warning: Could not create log directory {log_dir}: {e}", file=sys.stderr)
            # Continue without file logging if directory creation fails

    # Configure root logger
    # Remove previous handlers to avoid duplicate logs if script is run multiple times in same session
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    logging.basicConfig(
        level=log_level,
        format=log_format,
        filename=args.log_file, # Log to file
        filemode='w' # Overwrite log file each run
    )
    # Add console handler to also print INFO level messages and above to console
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO) # Console shows INFO and above
    formatter = logging.Formatter(log_format) # Can use the same format or a simpler one
    console_handler.setFormatter(formatter)
    logging.getLogger('').addHandler(console_handler)


    process_controls(args)

if __name__ == "__main__":
    main()
